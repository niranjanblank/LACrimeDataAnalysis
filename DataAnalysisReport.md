# LA Crime Data Analysis

## Introduction
Crime is a societal issue that impacts communities and cities around the world. Analyzing crime data helps to understand the nature, scale, and patterns of
criminal activities, enabling better decision-making in law enforcement and policy creation. This project focuses on exploratory analysis of crime 
 in Los Angeles, the second-most populous city in the United States, from 2020 to first quarter of 2023.

The objective of this analysis is to reveal patterns and trends in crime, understand the demographics of victims, determine peak times and locations for various crimes,
and gain other useful insights that can assist in developing effective crime prevention strategies.

Data for this project comes from [LA Crime Data](https://www.kaggle.com/datasets/chaitanyakck/crime-data-from-2020-to-present) available on Kaggle. The dataset contains
details about each reported crime event, including the date, type of crime, location, and demographics of the victim.

The primary tool used for this analysis tools were R, SQL and Python. R was used for cleaning the data using a library called `tidyverse`, which assists with data import, 
tidying and manipulation. SQL's powerful querying capabilities allow for complex analysis to be performed directly on the database. For further visualizations,
Python libraries such as pandas, matplotlib, and seaborn were used in addition to the dataframes generated by SQL queries.

The report includes detailed explanations of Data Cleaning Process, SQL queries used for the analysis, visualizations generated from the analysis, and discussions on the 
potential implications of the results.

## Data Cleaning and Preprocessing
Before delving into the analysis, an initial step of data cleaning and preprocessing was carried out to ensure the data quality and integrity. The tool of choice for this
phase was R, a language specifically designed for statistical analysis and data manipulation, along with its robust package 'tidyverse' which includes a collection of 
efficient tools for data cleaning.

The data set consist of the following columns related to the crime reported.
Sure, based on your provided column names and common conventions in crime datasets, here's a brief explanation of each:* DR_NO: This is likely a unique identifier for each crime report.

| Column Name | Description |
| ----------- | ----------- |
| Date_Rptd | The date when the crime was reported. |
| DATE_OCC | The date when the crime occurred. |
| TIME_OCC | The time when the crime occurred. |
| AREA | A numeric identifier for the area or district where the crime occurred. |
| AREA_NAME | The name of the area or district where the crime occurred. |
| Rpt_Dist_No | The reporting district number, which could refer to the specific police department or precinct that filed the report. |
| Part_1_2 | Indicates whether the crime is a Part I or Part II offense in the FBI's Uniform Crime Reporting (UCR) system. Part I offenses are major crimes like murder, rape, and robbery, while Part II offenses cover all other crimes. |
| Crm_Cd | A numeric identifier for the type of crime. |
| Crm_Cd_Desc | A description of the type of crime. |
| Mocodes | Modus Operandi codes, which provide details about the method or tactics the criminal used. |
| Vict_Age | The age of the victim. |
| Vict_Sex | The sex of the victim. |
| Vict_Descent | The descent or ethnicity of the victim. |
| Premis_Cd | A code representing the type of location where the crime occurred. |
| Premis_Desc | A description of the type of location where the crime occurred. |
| Weapon_Used_Cd | A code representing the type of weapon used in the crime. |
| Weapon_Desc | A description of the type of weapon used in the crime. |
| Status | A code representing the status of the investigation or case. |
| Status_Desc | A description of the status of the investigation or case. |
| Crm_Cd_1 | Additional crime code associated with the incident. |
| Crm_Cd_2 | Additional crime code associated with the incident. |
| Crm_Cd_3 | Additional crime code associated with the incident. |
| Crm_Cd_4 | Additional crime code associated with the incident. |
| LOCATION | The specific location where the crime occurred, potentially an address or specific landmark. |
| Cross_Street | The nearest cross-street or intersection to where the crime occurred. This is often used when an exact address isn't available or relevant. |
| LAT | The latitude coordinate of the crime location. |
| LON | The longitude coordinate of the crime location. |

Initially, the dataset was inspected for null values `colSums(is.na(crime_data))` to find total numm values in each column. It was found that several columns
contained a significant percentage of missing data. It is crucual to detal with missing values since they can skew the analysis and lead to incorrect conclusions.

Specifically, columns such as 'Mocodes'. 'Vict_Sex', 'Vict_Descent', 'Weapon_Used_Cd' and 'Weapon_Desc' has missing data. Rather than discarding these rows entirely,
which might have resulted in a significant loss of information, the decision was made to impute these missing values. For example categorical variables, missing value
were replaced with 'Unknown', allowing us to retain these records for analysis without creating misleading results.

For instance, 'Weapon_Used_Cd' and 'Weapon_Desc', despite having a high percentage of missing data, was considered important for the analysis.
The assumption was made that a missing value in 'Weapon_Used_Cd' and 'Weapon_Desc' indicated that the weapon was unknown, and therefore, these
were filled with 0 and 'Unknown' respectively.

On the other hand, the null values were set to 0 in case of 'Vict Age'. The negative values, which could be data entry errors were filtered during analysis
as they weren't in large quantity.

Other than that, 'Premis_Cd', 'Status Desc' and 'Crm Cd 1' had very few null values, which wouldn't make significant difference to this large dataset. Thus, rows with null values
in these columns were removed.

This preprocessing step produced a clean, structured dataset, setting a solid foundation for the ensuing exploratory data analysis and visualizations step.


















